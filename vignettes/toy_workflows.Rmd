---
title: "bifaToolkits package: toy-data workflows for each method"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bifaToolkits package: toy-data workflows for each method}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```


## 0. Install / load

Many methods are implemented in **optional backends** (GitHub/Bioconductor packages, or bundled scripts).

```{r eval=TRUE}
# install.packages("remotes")
# remotes::install_local("path/to/bifaToolkits")  # or install from your repo
library(bifaToolkits)
```

Check what you can run on your machine:

```{r eval=TRUE}
library(bifaToolkits)
available_backends()
```

If you need an optional backend (and it is unavailable above), install it via:

```{r eval=FALSE}
install_backend("bmsfa")        # installs MSFA from GitHub
install_backend("momss")        # installs BFR.BE + Bioc deps
install_backend("sufa")         # installs SUFA from GitHub (build_vignettes = FALSE)
install_backend("curated_ovarian")
```

and no need t `library` it after installing.

> **Note on MOM-SS/BFR.BE.**
> We only support MOM-SS in "BFR.BE" backend right now. If you want to run other method mentioned in their paper, eg. Laplace-SS, please refer to their original repository.

> **Note on SUFA.**  
> You need to install PROJ, sqlite3 and GDAL onto PATH. Several updates should be done, for example, terra. These might be difficult.

> **Note on BMSFA**
> install_backend("bmsfa") installs the MSFA package from Mavis's GitHub, which allows user-input scaling and centering options. The original MSFA package on CRAN does not have these options.

> **Note on script backends (PFA / Tetris).**  
> In this package, PFA and Tetris are bundled under `inst/extdata/pfa/` and `inst/extdata/tetris/`.  
> They are loaded on demand via `load_backend("pfa")` or `load_backend("tetris")`. PFA may compile `PFA.cpp`, which requires a working C++ toolchain.

**If install_backend function does not work, you can always install them yourself then run the bifaToolkits package.**

## 1. Load toy data (shipped in the package)

The toy data live in `inst/extdata/toy.rds`. You can load it using `system.file()`.

```{r eval=TRUE}
toy <- readRDS(system.file("extdata", "toy.rds", package = "bifaToolkits", mustWork = TRUE))
str(toy, max.level = 2)
```

The package wrappers expect `Y_list`: a list of study matrices with the same number of columns (variables).


```{r eval=TRUE}
Y_list <- toy$Y_list
S <- length(Y_list)
P <- ncol(Y_list[[1]])
c(S = S, P = P)
```

Throughout this vignette:
- `S` = number of studies
- `P` = number of variables

## 2. Common post-processing outputs

Each `postprocess_*()` function returns a list of covariance components used in the tutorial:

- `Phi` / `SigmaPhi`: shared loadings / shared covariance
- `LambdaList` / `SigmaLambdaList`: study-specific loadings / covariances (if the method has them)
- `Psi` or `PsiList`: diagonal residual variances
- `SigmaMarginal`: per-study marginal covariance (shared + specific + residual)

The selection helpers choose dimensions using an **eigenvalue proportion rule** (see `select_k_from_sigma()`), and the `refit_*()` helpers
wrap the “initial fit → select dims → refit” workflow.

---

# Method A: BMSFA (MSFA backend)

## A1. Fit

You specify:
- `k`: number of shared factors
- `j_s`: number of study-specific factors (length `S`)

You can control MCMC via `control = list(nrun = ..., burn = ...)`.

```{r eval=TRUE}
fit0 <- fit_bmsfa(
  Y_list = Y_list,
  k = 5, # Overspecified
  j_s = rep(2, S),
  centering = TRUE,
  scaling = FALSE,
  control = list(nrun = 1000, burn = 200)
)
```

## A2. Post-process

```{r eval=TRUE}
post0 <- postprocess_bmsfa(fit0)

names(post0)
```

## A3. Select dimensions and refit

```{r eval=TRUE}
select_k_bmsfa(post0, cutoff = 0.05)
select_js_bmsfa(post0, cutoff = 0.05)

# refit
out <- fit_bmsfa_2step(
  Y_list = Y_list,
  post_fit0 = post0,
  cutoff = 0.05,
  control = list(nrun = 1000, burn = 200)
)
# Or `fit_bmsfa` with the selected k and j_s, and `postprocess` again.
```

## A4. Fit two-step BMSFA in one call

```{r eval=TRUE}
out <- fit_bmsfa_2step(
  Y_list = Y_list,
  k = 5, # Overspecified
  j_s = rep(2, S),
  centering = TRUE,
  scaling = FALSE,
  control = list(nrun = 1000, burn = 200)
)
```

---

# Method B: Stack FA (MSFA backend)

“Stack FA” stacks all studies and fits a single factor model.

## B1. Fit

```{r eval=TRUE}
fit0 <- fit_stack_fa(
  Y_list = Y_list,
  k = 5, # Overspecified
  centering = TRUE,
  scaling = FALSE,
  control = list(nrun = 1000, burn = 200)
)
```

## B2. Post-process

`postprocess_stack_fa()` takes `S` so it can return a per-study list for `SigmaMarginal`.

```{r eval=TRUE}
post0 <- postprocess_stack_fa(fit0, S = S)
names(post0)
```

## B3. Select K and refit

```{r eval=TRUE}
select_k_stack_fa(post0, cutoff = 0.05)

out <- fit_stack_fa_2step(
  Y_list = Y_list,
  post_fit0 = post0,
  cutoff = 0.05,
  control = list(nrun = 1000, burn = 200)
)
```

## B4. Fit two-step Stack FA in one call

```{r eval=TRUE}
out <- fit_stack_fa_2step(
  Y_list = Y_list,
  k = 4,# Overspecified
  control = list(nrun = 1000, burn = 200)  
)
```

---

# Method C: Ind FA (MSFA backend)

“Ind FA” fits separate factor models per study.

## C1. Fit

You specify `j_s` as either:
- a single integer (recycled to all studies), or
- a length-`S` integer vector.

```{r eval=TRUE}
fit0 <- fit_ind_fa(
  Y_list = Y_list,
  j_s = rep(3, S),
  centering = TRUE,
  scaling = FALSE,
  control = list(nrun = 1000, burn = 200)
)
```

## C2. Post-process, select J_s, refit

```{r eval=TRUE}
post0 <- postprocess_ind_fa(fit0)
select_js_ind_fa(post0, cutoff = 0.05)

out <- fit_ind_fa_2step(
  Y_list = Y_list,
  post_fit0 = post0,
  cutoff = 0.05,
  control = list(nrun = 1000, burn = 200)
)
```

## C3. Fit two-step Ind FA in one call

```{r eval=TRUE}
out <- fit_ind_fa_2step(
  Y_list = Y_list,
  j_s = rep(3, S),
  centering = TRUE,
  scaling = FALSE,
  control = list(nrun = 1000, burn = 200)
)
```

---

# Method D: MOM-SS (BFR.BE backend)

MOM-SS uses the `BFR.BE` backend and requires:
- `q` (aka `k`): number of shared factors
- optional covariates `X` (passed to the backend as `v`)

**In this package wrapper**, if you pass `Y_list`, the wrapper stacks it into `x`, builds the **membership matrix** `b`,
and forwards `q` and `v`.

## D1. Prepare covariates (optional)

`X` can be either:
- a single matrix with `sum(nrow(Y_list))` rows, or
- a list of matrices aligned to `Y_list` (which will be stacked).

Example: two synthetic covariates per subject:

```{r eval=TRUE}
X_list <- lapply(Y_list, function(Y) {
  n <- nrow(Y)
  cbind(x1 = rnorm(n), x2 = rnorm(n))
})
```

## D2. Fit and post-process

```{r eval=TRUE}
fit <- fit_momss(
  Y_list = Y_list,
  k = 6,      
  X = X_list, # optional
  scaling = FALSE
)
post <- postprocess_momss(fit)

names(post)
# post$SigmaPhi, post$Psi, post$alpha, post$B, post$SigmaMarginal
```

---

# Method E: SUFA (SUFA backend)

In the tutorial workflow, we typically:
- center within each study (`center = TRUE`)
- do not rescale variances (`scale = FALSE`)
- specify `qmax` and `nrun`

## E1. Fit and post-process

```{r eval=FALSE, message=FALSE}
#install_backend("sufa")
fit <- fit_sufa(
  Y_list = Y_list,
  k = 6,      # or kmax = 6 depending on your wrapper
  nrun = 1000,
  center = TRUE,
  scale = FALSE
)
post <- postprocess_sufa(fit) 

names(post)
```

---

# Method F: PFA (script backend)

PFA scripts are bundled in `inst/extdata/pfa/`. The wrapper:
1) loads the backend via `load_backend("pfa")`  
2) centers/scales each study (defaults `center=TRUE`, `scale=FALSE`)  
3) stacks into one matrix `X` and builds the study id vector `b`  
4) calls the backend `PFA(X = X, b = b, k = ..., ...)`

## F1. Fit

```{r eval=TRUE}
fit <- fit_pfa(
  Y_list = Y_list,
  k = 6, # Overspecified
  center = TRUE,
  scale = FALSE,
  nrun = 1000,
  burn = 500
  # other backend parameters via ...
)
```

## F2. Post-process and select K

```{r eval=TRUE}
post <- postprocess_pfa(fit)
select_k_pfa(post)
```

---

# Method G: Tetris (script backend)

Tetris scripts are bundled in `inst/extdata/tetris/`.

The “3-step” workflow from the tutorial is:

1) initial run: `tetris(..., fixed_bigT = FALSE)`  
2) select `big_T`: `choose.A(fit, alpha_IBP = alpha, S = S)`  
3) fixed run: `tetris(..., fixed_bigT = TRUE, bigT = big_T)`


## G1. Manual 3-step pipeline

```{r eval=TRUE, warning=FALSE, message=FALSE}
fit1 <- fit_tetris(Y_list, alpha = "auto", beta = 1, fixed_bigT = FALSE, nrun = 500, burn = 200, nprint = 100) # Reduced iterations a faster runtime
big_T <- select_T_tetris(fit1)  # passes alpha_IBP and S from fit1 metadata by default
fit2 <- fit_tetris(Y_list, alpha = fit1$meta$alpha, beta = 1, fixed_bigT = TRUE, bigT = big_T,
                   nrun = 500, burn = 200, nprint = 100)
out <- postprocess_tetris(fit2)
```

## G2. One-shot pipeline (recommended)

```{r eval=TRUE, warning=FALSE, message=FALSE}
out <- fit_tetris_2step(
  Y_list = Y_list,
  alpha = "auto",   # ceiling(1.25 * S)
  beta = 1,
  nrun = 500,
  burn = 200,
  nprint = 100
)
```

---

# Method H: One-call dispatch that can invoke any method

- `fit_integrative_fa(method = "bmsfa"|"momss"|..., ...)`
- `postprocess_integrative_fa(method = ..., fit)`

Example:

```{r eval=TRUE}
fit <- fit_integrative_fa("bmsfa", Y_list = Y_list, k = 4, j_s = rep(2, S))
post <- postprocess_integrative_fa("bmsfa", fit)
```
# Visuallization

This function give a nice heatmap plot for the loadings

```{r eval=TRUE}
plot_single_loadings(mat = out$Phi, fill_limits = c(-2, 2)) # Displays grey if values are out of range
```

---

## Practical tips

- Start with **short runs** (`nrun`/`burn`) to validate your pipeline, then increase.
- Always check that each study has the same `P` (number of columns).
- For methods with rotational ambiguity, rely on the package post-processing (e.g., OP alignment) before comparing loadings.

## Session info

```{r eval=TRUE}
sessionInfo()
```
